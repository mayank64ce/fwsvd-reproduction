{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0o2M_THl40H"
      },
      "source": [
        "#1. Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fS-I3UaJ0Cw7"
      },
      "outputs": [],
      "source": [
        "# !pip install -U datasets\n",
        "# !pip install lm-eval==0.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E0G3nmxGlwfe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "# from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import time\n",
        "from lm_eval.base import BaseLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M_11G8VZ6Uza"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class Args:\n",
        "  alpha: float = 0.5\n",
        "  n_calib_samples: int = 16\n",
        "  rank_align: int = 1\n",
        "  use_cache: bool = True\n",
        "  param_ratio_target: float = 0.5\n",
        "  ppl_target: float = 40\n",
        "  act_aware: bool = True\n",
        "  sigma_fuse: str = \"UV\"\n",
        "  model_id: str = \"facebook/opt-125m\"\n",
        "  use_bos: bool = False\n",
        "  eval_ppl: str = \"wikitext2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7y4J-l1VMt4H"
      },
      "outputs": [],
      "source": [
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3v43QisPmAs0"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(args.model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAqJqztYNFwj"
      },
      "source": [
        "## Eval utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XO4BnFy_HupP"
      },
      "outputs": [],
      "source": [
        "class EvalLM(BaseLM):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        # device=\"cuda:0\",\n",
        "        batch_size=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # assert isinstance(device, str)\n",
        "        assert isinstance(batch_size, int)\n",
        "\n",
        "        # self._device = torch.device(device)\n",
        "        self._device = model.device\n",
        "\n",
        "        # self.model = model.to(self.device)\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "        self.batch_size_per_gpu = batch_size  # todo: adaptive batch size\n",
        "\n",
        "        self.seqlen = 2048\n",
        "\n",
        "    @property\n",
        "    def eot_token_id(self):\n",
        "        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n",
        "        return self.tokenizer.eos_token_id\n",
        "\n",
        "    @property\n",
        "    def max_length(self):\n",
        "        try:\n",
        "            return self.model.config.n_ctx\n",
        "        except AttributeError:\n",
        "            # gptneoconfig doesn't have n_ctx apparently\n",
        "            return self.model.config.max_position_embeddings\n",
        "\n",
        "    @property\n",
        "    def max_gen_toks(self):\n",
        "        return 256\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        # TODO: fix multi-gpu\n",
        "        return self.batch_size_per_gpu  # * gpus\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        # TODO: fix multi-gpu\n",
        "        return self._device\n",
        "\n",
        "    def tok_encode(self, string: str):\n",
        "        return self.tokenizer.encode(string, add_special_tokens=False)\n",
        "\n",
        "    def tok_decode(self, tokens):\n",
        "        return self.tokenizer.decode(tokens)\n",
        "\n",
        "    def _model_call(self, inps):\n",
        "        \"\"\"\n",
        "        inps: a torch tensor of shape [batch, sequence]\n",
        "        the size of sequence may vary from call to call\n",
        "\n",
        "        returns: a torch tensor of shape [batch, sequence, vocab] with the\n",
        "        logits returned from the model\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model(inps)[0][:, :, :50257]\n",
        "\n",
        "    def _model_generate(self, context, max_length, eos_token_id):\n",
        "        return self.model.generate(context, max_length=max_length, eos_token_id=eos_token_id, do_sample=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VutzTj9iLzBh"
      },
      "outputs": [],
      "source": [
        "def get_eval_loaders(name, tokenizer):\n",
        "    if \"wikitext2\" in name:\n",
        "        testdata = load_dataset(\n",
        "            \"wikitext\",\n",
        "            \"wikitext-2-raw-v1\",\n",
        "            split=\"test\",\n",
        "        )\n",
        "        testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")\n",
        "        return testenc\n",
        "    if \"ptb\" in name:\n",
        "        valdata = load_dataset(\n",
        "            \"ptb_text_only\",\n",
        "            \"penn_treebank\",\n",
        "            split=\"validation\",\n",
        "        )\n",
        "        testenc = tokenizer(\"\\n\\n\".join(valdata[\"sentence\"]), return_tensors=\"pt\")\n",
        "        return testenc\n",
        "    if \"c4\" in name:\n",
        "        testdata = load_dataset(\n",
        "            \"allenai/c4\",\n",
        "            \"allenai--c4\",\n",
        "            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n",
        "            split=\"validation\",\n",
        "        )\n",
        "        testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")\n",
        "        return testenc\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aho8Nk0nLBL7"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    model_name,\n",
        "    eval_ppl=\"\",\n",
        "    num_fewshot=0,\n",
        "    limit=-1,\n",
        "    batch_size=1,\n",
        "    use_bos=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    model: model name\n",
        "    limit: number of test samples for debug, set to -1 is no limit\n",
        "    tasks: str tasks are split by ,\n",
        "    num_fewshot: Number of examples in few-shot context\n",
        "    eval_ppl: str datasets are split by , such as 'wikitext2,ptb,c4'\n",
        "    \"\"\"\n",
        "    lm = EvalLM(model, tokenizer, batch_size=batch_size)\n",
        "\n",
        "    results = {}\n",
        "    if eval_ppl:\n",
        "        for dataset in eval_ppl.split(\",\"):\n",
        "            cache_testloader = f\"/tmp/{dataset}_testloader_{model_name.replace('/', '_')}_all.cache\"\n",
        "            if os.path.exists(cache_testloader):\n",
        "                testloader = torch.load(cache_testloader, weights_only=False)\n",
        "                # print(f\"load calibration from {cache_testloader}\")\n",
        "            else:\n",
        "                testloader = get_eval_loaders(dataset, tokenizer)\n",
        "                torch.save(testloader, cache_testloader)\n",
        "            # print(dataset)\n",
        "            testenc = testloader.input_ids\n",
        "            if use_bos:\n",
        "                lm.seqlen -= 1\n",
        "            nsamples = testenc.numel() // lm.seqlen\n",
        "            use_cache = lm.model.config.use_cache\n",
        "            lm.model.config.use_cache = False\n",
        "            lm.model.eval()\n",
        "            nlls = []\n",
        "\n",
        "            for i in tqdm(range(nsamples)):\n",
        "                batch = testenc[:, (i * lm.seqlen) : ((i + 1) * lm.seqlen)].to(lm.device)\n",
        "                if use_bos:\n",
        "                    bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * batch.size(dim=0)).to(lm.device)\n",
        "                    batch = torch.cat([bos_tokens_tensor, batch], dim=1)\n",
        "                outputs = lm.model.model(batch)\n",
        "                hidden_states = outputs[0]  # .to(lm.model.lm_head.weight.device)\n",
        "                if use_bos:\n",
        "                    hidden_states = hidden_states[:, 1:, :]\n",
        "                logits = lm.model.lm_head(hidden_states)  # .contiguous()\n",
        "                shift_logits = logits[:, :-1, :]  # .contiguous()\n",
        "                shift_labels = testenc[:, (i * lm.seqlen) : ((i + 1) * lm.seqlen)][:, 1:].to(lm.device)\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(\n",
        "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                    shift_labels.view(-1),\n",
        "                )\n",
        "                neg_log_likelihood = loss.float() * lm.seqlen\n",
        "                nlls.append(neg_log_likelihood)\n",
        "                if i == limit:\n",
        "                    break\n",
        "                # if i == 1:\n",
        "                #     print(\n",
        "                #         \"memory_allocated\",\n",
        "                #         i,\n",
        "                #         torch.cuda.memory_allocated() / 1024 / 1024,\n",
        "                #         \"max memory_allocated\",\n",
        "                #         torch.cuda.max_memory_allocated() / 1024**2,\n",
        "                #     )\n",
        "\n",
        "            ppl = torch.exp(torch.stack(nlls).sum() / (len(nlls) * lm.seqlen))\n",
        "            print(dataset, ppl.item())\n",
        "            lm.model.config.use_cache = use_cache\n",
        "            results[dataset] = ppl.item()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_JWLlbINftW"
      },
      "source": [
        "## Evaluation before compression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "918191b03b344e79b351cfc01bbe2804",
            "2464a177884f4565a00e2bd5e6a1b308",
            "ac41383d3a5f46e3882417bbd8aca034",
            "b8c7460d407948a2a04ab4373969482e",
            "772ed6cb50ed4bd885e1cd43fdd5f312",
            "24d4de4c7f384524a9bba3bd466b66c7",
            "416cc21dbd814802a11150d75b7dfa8f",
            "ec67c4a1f82a45458707e7562c67f576",
            "9f57359032d74a87b6a0103d2cd6e4f1",
            "3c0768d72c0a4bdc845a0d43dc9dbdb8",
            "8dc70d80618b4bc7a5ed241d6146d47c"
          ]
        },
        "id": "i7CCxl0hNkTn",
        "outputId": "e0f342ed-03d8-4233-bd71-7470f4d776ee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "918191b03b344e79b351cfc01bbe2804",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/140 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wikitext2 27.653562545776367\n",
            "{'wikitext2': 27.653562545776367}\n"
          ]
        }
      ],
      "source": [
        "result = evaluate_model(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        args.model_id,\n",
        "        eval_ppl=args.eval_ppl,\n",
        "        limit=-1,\n",
        "        use_bos=args.use_bos,\n",
        "    )\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRC6o-WmnTDO"
      },
      "source": [
        "#2. Load calibration data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UJ8GHFItnVmh"
      },
      "outputs": [],
      "source": [
        "# we will use c4 as calibration data\n",
        "def get_calib_data(tokenizer, model_id, nsamples=16, seqlen=2048, seed=3, use_bos=False):\n",
        "  cache_file = f\"cache/c4_{model_id.replace('/','_')}_{nsamples}_{seqlen}_{seed}_bos{use_bos}.pt\"\n",
        "  print(f\"cache_file={cache_file}\")\n",
        "  if not os.path.exists(\"cache\"):\n",
        "        os.makedirs(\"cache\")\n",
        "  if os.path.exists(cache_file):\n",
        "      traindataset = torch.load(cache_file)\n",
        "      return traindataset\n",
        "\n",
        "  traindata = load_dataset(\n",
        "            \"allenai/c4\", data_files={\"train\": \"en/c4-train.00000-of-01024.json.gz\"}, split=\"train\"\n",
        "  )\n",
        "  tot_text = \"\\n\\n\".join(traindata[\"text\"])\n",
        "  print(f\"tot_text={len(tot_text)}\")\n",
        "  traindataset = []\n",
        "  for _ in range(nsamples):\n",
        "      i = random.randint(0, len(tot_text) - seqlen - 1)\n",
        "      j = i + seqlen * 10\n",
        "      txt = tot_text[i:j]\n",
        "      ind = txt.find(\".\")\n",
        "      txt = txt[ind + 1 :].strip()\n",
        "      if use_bos:\n",
        "          txt = tokenizer.bos_token + txt\n",
        "      trainenc = tokenizer(txt, return_tensors=\"pt\")\n",
        "      inp = trainenc.input_ids[:, :seqlen]\n",
        "      attention_mask = torch.ones_like(inp)\n",
        "      traindataset.append({\"input_ids\": inp, \"attention_mask\": attention_mask})\n",
        "  torch.save(traindataset, cache_file)\n",
        "  return traindataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqmUjqfjoTlc",
        "outputId": "1a615aad-f7d4-4341-f3c3-799dcc8ab1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cache_file=cache/c4_facebook_opt-125m_16_2048_3_bosFalse.pt\n"
          ]
        }
      ],
      "source": [
        "calib_loader = get_calib_data(tokenizer, model.name_or_path, args.n_calib_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA6jZnSt1uN3"
      },
      "source": [
        "#3. Compute Fisher Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HaJ7p8HY1t0E"
      },
      "outputs": [],
      "source": [
        "def calib_fisher_info(model, calib_loader, use_cache=True):\n",
        "  model_id = model.config.name_or_path\n",
        "  cache_file = f\"cache/{model_id.replace('/','_')}_calib_fisher_info.pt\"\n",
        "\n",
        "  if os.path.exists(cache_file) and use_cache:\n",
        "        all_fisher_info = torch.load(cache_file, map_location=\"cpu\")\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                module.fisher_info = all_fisher_info[name].to(module.weight.device)\n",
        "        return\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for name, module in model.named_modules():\n",
        "      if isinstance(module, nn.Linear):\n",
        "          module.fisher_info = 0\n",
        "\n",
        "  # get fisher info\n",
        "  for batch in tqdm(calib_loader):\n",
        "      input_ids = batch[\"input_ids\"][:, :-1].to(model.device)\n",
        "      labels = batch[\"input_ids\"][:, 1:].to(model.device)\n",
        "      out = model(input_ids=input_ids, labels=labels)\n",
        "      out[0].backward()\n",
        "      for name, module in model.named_modules():\n",
        "          if isinstance(module, nn.Linear):\n",
        "              module.fisher_info += module.weight.grad.detach().pow(2).mean(0) # mean over (dL/dw)**2 (why a mean here ?) (should it be sum instead ?)\n",
        "      model.zero_grad()\n",
        "\n",
        "  for name, module in model.named_modules():\n",
        "      if isinstance(module, nn.Linear):\n",
        "          module.fisher_info = module.fisher_info.div(len(calib_loader)).sqrt() # dividing by length of dataset and square root.\n",
        "\n",
        "  # remove and save fisher_info\n",
        "  all_fisher_info = {}\n",
        "  for name, module in model.named_modules():\n",
        "      if isinstance(module, nn.Linear):\n",
        "          module._forward_hooks.clear()\n",
        "          all_fisher_info[name] = module.fisher_info\n",
        "  torch.save(all_fisher_info, cache_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kmnrXpeooi4t"
      },
      "outputs": [],
      "source": [
        "calib_fisher_info(model, calib_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndZ4UPNx5MF-"
      },
      "source": [
        "#4. Use Fisher Information to compress the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5BQHawLu8F0U"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_perplexity(model, dataset, limit):\n",
        "    \"\"\"\n",
        "    dataset: input ids tensor of shape [batch, sequence length]\n",
        "    \"\"\"\n",
        "    nsamples, seqlen = dataset.size()\n",
        "\n",
        "    nlls = []\n",
        "\n",
        "    for i in range(nsamples):\n",
        "        if i == limit:\n",
        "            break\n",
        "        input_ids = dataset[i : i + 1, :-1].to(model.device)\n",
        "        labels = dataset[i : i + 1, 1:].contiguous()\n",
        "        logits = model(input_ids=input_ids)[0]\n",
        "        shift_logits = logits[:, :, :]\n",
        "        shift_labels = labels.to(model.device)\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),\n",
        "            shift_labels.view(-1),\n",
        "        )\n",
        "        neg_log_likelihood = loss.float() * seqlen\n",
        "        nlls.append(neg_log_likelihood)\n",
        "    ppl = torch.exp(torch.stack(nlls).sum() / (len(nlls) * seqlen))\n",
        "    return ppl.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "C4oLlJnZ7ayX"
      },
      "outputs": [],
      "source": [
        "class SVDLinear(nn.Module):\n",
        "    def __init__(self, U, S, V, bias=None, sigma_fuse=\"UV\") -> None:\n",
        "        super().__init__()\n",
        "        self.ALinear = nn.Linear(U.size(1), U.size(0), bias=bias is not None)\n",
        "\n",
        "        if bias is not None:\n",
        "            self.ALinear.bias.data = bias\n",
        "        self.BLinear = nn.Linear(V.size(1), V.size(0), bias=False)\n",
        "        self.truncation_rank = S.size(0)\n",
        "        if sigma_fuse == \"UV\":\n",
        "            self.ALinear.weight.data = U.mul(S.sqrt()).contiguous()\n",
        "            self.BLinear.weight.data = V.t().mul(S.sqrt().view(-1, 1)).contiguous()\n",
        "        elif sigma_fuse == \"U\":\n",
        "            self.ALinear.weight.data = U.mul(S).contiguous()\n",
        "            self.BLinear.weight.data = V.t().contiguous()\n",
        "        elif sigma_fuse == \"V\":\n",
        "            self.ALinear.weight.data = U.contiguous()\n",
        "            self.BLinear.weight.data = V.t().mul(S.view(-1, 1)).contiguous()\n",
        "\n",
        "    @staticmethod\n",
        "    def from_linear(\n",
        "        linear: nn.Linear,\n",
        "        param_ratio: float,\n",
        "        act_aware=False,\n",
        "        ic_split=1,\n",
        "        oc_split=1,\n",
        "        alpha=1,\n",
        "        sigma_fuse=\"UV\",\n",
        "        rank_align=1,\n",
        "    ):\n",
        "        # if param_ratio >= 1:\n",
        "        #     return linear\n",
        "        n_params = linear.weight.numel()\n",
        "        compressed_params = int(n_params * param_ratio)\n",
        "        assert ic_split == 1 or oc_split == 1\n",
        "        rank = compressed_params // (linear.in_features + linear.out_features)\n",
        "        # rank align\n",
        "        rank = int(np.ceil(rank / rank_align) * rank_align)\n",
        "\n",
        "        # print(\"rank\", rank)\n",
        "        w = linear.weight.data.float()\n",
        "        if act_aware:\n",
        "            scaling_diag_matrix = 1  # avoid zero division\n",
        "            if hasattr(linear, \"scaling_diag_matrix\"):\n",
        "                # print(\"WARNING: scaling_diag_matrix is used\")\n",
        "                scaling_diag_matrix *= linear.scaling_diag_matrix**alpha\n",
        "                # scaling_diag_matrix *= linear.scaling_diag_matrix**0.5\n",
        "            if hasattr(linear, \"fisher_info\"):\n",
        "                scaling_diag_matrix *= linear.fisher_info**alpha\n",
        "                # scaling_diag_matrix *= linear.fisher_info**1\n",
        "            # if not (scaling_diag_matrix == scaling_diag_matrix).all():\n",
        "            if not isinstance(scaling_diag_matrix, torch.Tensor):\n",
        "                breakpoint()\n",
        "            scaling_diag_matrix += 1e-6  # avoid zero division\n",
        "            w = w * scaling_diag_matrix.view(1, -1)\n",
        "        Us = []\n",
        "        Ss = []\n",
        "        Vs = []\n",
        "        try:\n",
        "            U, S, V = torch.svd_lowrank(w, q=rank)\n",
        "        except:\n",
        "            print(f\"svd failed for {linear}, disable act_aware\")\n",
        "            return nn.Linear(linear.in_features, linear.out_features).to(linear.weight.dtype).to(linear.weight.device)\n",
        "        if act_aware:\n",
        "            V = V / scaling_diag_matrix.view(-1, 1)\n",
        "        Us = [U]\n",
        "        Ss = [S]\n",
        "        Vs = [V]\n",
        "\n",
        "        if linear.bias is not None:\n",
        "            bias = linear.bias.data\n",
        "        else:\n",
        "            bias = None\n",
        "\n",
        "        # nan or inf check\n",
        "        for S in Ss:\n",
        "            if (S != S).any():\n",
        "                print(\"nan in S\")\n",
        "                return (\n",
        "                    nn.Linear(linear.in_features, linear.out_features).to(linear.weight.dtype).to(linear.weight.device)\n",
        "                )\n",
        "        for U in Us:\n",
        "            if (U != U).any():\n",
        "                print(\"nan in U\")\n",
        "                return (\n",
        "                    nn.Linear(linear.in_features, linear.out_features).to(linear.weight.dtype).to(linear.weight.device)\n",
        "                )\n",
        "        for V in Vs:\n",
        "            if (V != V).any():\n",
        "                print(\"nan in V\")\n",
        "                return (\n",
        "                    nn.Linear(linear.in_features, linear.out_features).to(linear.weight.dtype).to(linear.weight.device)\n",
        "                )\n",
        "\n",
        "        assert len(Us) == len(Ss) == len(Vs) == 1\n",
        "        new_linear = SVDLinear(Us[0], Ss[0], Vs[0], bias, sigma_fuse)\n",
        "        new_linear.to(linear.weight.dtype)\n",
        "        return new_linear\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # compute USV^Tx + b\n",
        "        y = self.BLinear(inp)\n",
        "        y = self.ALinear(y)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GdyQbM8J4yLv"
      },
      "outputs": [],
      "source": [
        "# Calibrate sensitivity\n",
        "\n",
        "@torch.no_grad()\n",
        "def calib_sensitivity_ppl(model, calib_loader, args, use_cache=True):\n",
        "  model_id = model.config._name_or_path\n",
        "  cache_file = f\"cache/{model_id.replace('/','_')}_sensitivity_fisher_{args.alpha}_{args.n_calib_samples}_c4.pt\"\n",
        "  if os.path.exists(cache_file) and use_cache:\n",
        "      sensitivity_dict = torch.load(cache_file, map_location=\"cpu\")\n",
        "      return sensitivity_dict\n",
        "\n",
        "  model.eval()\n",
        "  full_name_dict = {module: name for name, module in model.named_modules()}\n",
        "  linear_info = {}\n",
        "  modules = [model]\n",
        "  while len(modules) > 0:\n",
        "      submodule = modules.pop()\n",
        "      for name, raw_linear in submodule.named_children():\n",
        "          if isinstance(raw_linear, nn.Linear):\n",
        "              full_name = full_name_dict[raw_linear]\n",
        "              linear_info[raw_linear] = {\n",
        "                  \"father\": submodule,\n",
        "                  \"name\": name,\n",
        "                  \"full_name\": full_name,\n",
        "              }\n",
        "          else:\n",
        "              modules.append(raw_linear)\n",
        "\n",
        "  sensitivity_dict = {}\n",
        "\n",
        "  param_ratio_candidates = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "  input_ids = torch.cat([_[\"input_ids\"] for _ in calib_loader], 0)\n",
        "  print(f\"input_ids.shape={input_ids.shape}\")\n",
        "  pbar = tqdm(total=len(linear_info) * len(param_ratio_candidates))\n",
        "  for raw_linear, info in linear_info.items():\n",
        "      sensitivity_dict[info[\"full_name\"]] = {}\n",
        "      for param_ratio in param_ratio_candidates:\n",
        "          svd_linear = SVDLinear.from_linear(\n",
        "              raw_linear,\n",
        "              param_ratio=param_ratio,\n",
        "              alpha=args.alpha,\n",
        "              act_aware=True,\n",
        "              rank_align=args.rank_align,\n",
        "          )\n",
        "          setattr(info[\"father\"], info[\"name\"], svd_linear)\n",
        "\n",
        "          ppl = evaluate_perplexity(model, input_ids, args.n_calib_samples)\n",
        "          sensitivity_dict[info[\"full_name\"]][param_ratio] = ppl\n",
        "          print(f\"{info['full_name']} {param_ratio} {ppl}\")\n",
        "          pbar.update(1)\n",
        "      setattr(info[\"father\"], info[\"name\"], raw_linear)\n",
        "  torch.save(sensitivity_dict, cache_file)\n",
        "  return sensitivity_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6KKxEdBR8JmX"
      },
      "outputs": [],
      "source": [
        "sensitivity = calib_sensitivity_ppl(model, calib_loader, args, args.use_cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pCIwMtFqDlR-"
      },
      "outputs": [],
      "source": [
        "def binary_search_truncation_rank(model, sensitivity_dict, calib_loader, args):\n",
        "    module_dict = {name: module for name, module in model.named_modules()}\n",
        "    full_name_dict = {module: name for name, module in model.named_modules()}\n",
        "    linear_info = {}\n",
        "    modules = [model]\n",
        "    while len(modules) > 0:\n",
        "        submodule = modules.pop()\n",
        "        for name, raw_linear in submodule.named_children():\n",
        "            if isinstance(raw_linear, nn.Linear):\n",
        "                full_name = full_name_dict[raw_linear]\n",
        "\n",
        "                linear_info[raw_linear] = {\n",
        "                    \"father\": submodule,\n",
        "                    \"name\": name,\n",
        "                    \"full_name\": full_name,\n",
        "                }\n",
        "            else:\n",
        "                modules.append(raw_linear)\n",
        "\n",
        "\n",
        "    ratio_target = args.param_ratio_target\n",
        "    default_param_ratio = 1\n",
        "\n",
        "    print(\n",
        "        f\"=== target: ppl={args.ppl_target}, ratio_target={ratio_target} ===\"\n",
        "    )\n",
        "\n",
        "    sensitivity_list = []\n",
        "    for layername, v in sensitivity_dict.items():\n",
        "        for param_ratio, ppl in v.items():\n",
        "            if param_ratio >= 1:\n",
        "                # we need to compress the weights, so parameter ratio should be less than 1\n",
        "                continue\n",
        "            sensitivity_list.append((layername, param_ratio, ppl))\n",
        "    sorted_sensitive_list = sorted(sensitivity_list, key=lambda x: -x[2])\n",
        "\n",
        "    # binary search\n",
        "    high = len(sorted_sensitive_list) - 1\n",
        "    low = 0\n",
        "    assert args.ppl_target > 0 or ratio_target > 0\n",
        "\n",
        "    input_ids = torch.cat([_[\"input_ids\"] for _ in calib_loader], 0)\n",
        "    while low < high:\n",
        "        mid = (low + high) // 2\n",
        "        layers_min_ratio = {layername: default_param_ratio for layername in sensitivity_dict.keys()}\n",
        "        for layername, param_ratio, ppl in sorted_sensitive_list[mid:]:\n",
        "            layers_min_ratio[layername] = min(layers_min_ratio[layername], param_ratio)\n",
        "        tot_params = 0\n",
        "        compress_params = 0\n",
        "        if args.ppl_target > 0:\n",
        "            # assert not args.compress_kv_cache, \"ppl_target is not supported when compressing kv_cache now\"\n",
        "            for layername, param_ratio in layers_min_ratio.items():\n",
        "                raw_linear = module_dict[layername]\n",
        "                info = linear_info[raw_linear]\n",
        "                svd_linear = SVDLinear.from_linear(\n",
        "                    raw_linear,\n",
        "                    param_ratio=param_ratio,\n",
        "                    alpha=args.alpha,\n",
        "                    act_aware=args.act_aware,\n",
        "                    sigma_fuse=args.sigma_fuse,\n",
        "                    rank_align=args.rank_align,\n",
        "                )\n",
        "                setattr(info[\"father\"], info[\"name\"], svd_linear)\n",
        "                tot_params += raw_linear.weight.numel()\n",
        "                compress_params += raw_linear.weight.numel() * param_ratio\n",
        "            ppl = evaluate_perplexity(model, input_ids, args.n_calib_samples)\n",
        "            param_ratio = compress_params / tot_params\n",
        "            msg = f\"low={low} mid={mid}, high={high}, ppl={ppl}, param_ratio={param_ratio}\"\n",
        "            print(msg)\n",
        "            if ppl < args.ppl_target:\n",
        "                high = mid\n",
        "            else:\n",
        "                low = mid + 1\n",
        "        else:\n",
        "            for layername, param_ratio in layers_min_ratio.items():\n",
        "                raw_linear = module_dict[layername]\n",
        "                tot_params += raw_linear.weight.numel()\n",
        "                compress_params += raw_linear.weight.numel() * param_ratio\n",
        "            now_ratio = compress_params / tot_params\n",
        "\n",
        "            msg = f\"low={low} mid={mid}, high={high}, now_ratio={now_ratio}, params=({compress_params}/{tot_params})\"\n",
        "            print(msg)\n",
        "            if now_ratio > ratio_target:\n",
        "                high = mid\n",
        "            else:\n",
        "                low = mid + 1\n",
        "\n",
        "    print(f\"=== Searching done, decomposing layers... ===\")\n",
        "    layers_min_ratio = {layername: default_param_ratio for layername in sensitivity_dict.keys()}\n",
        "    for layername, param_ratio, ppl in sorted_sensitive_list[mid:]:\n",
        "        if layers_min_ratio[layername] is None:\n",
        "            layers_min_ratio[layername] = param_ratio\n",
        "        else:\n",
        "            layers_min_ratio[layername] = min(layers_min_ratio[layername], param_ratio)\n",
        "    st = time.time()\n",
        "    for layername, param_ratio in tqdm(layers_min_ratio.items()):\n",
        "        # set ratio\n",
        "        raw_linear = module_dict[layername]\n",
        "        info = linear_info[raw_linear]\n",
        "        if param_ratio == default_param_ratio:\n",
        "            svd_linear = raw_linear\n",
        "        else:\n",
        "            svd_linear = SVDLinear.from_linear(\n",
        "                raw_linear,\n",
        "                param_ratio=param_ratio,\n",
        "                alpha=args.alpha,\n",
        "                act_aware=args.act_aware,\n",
        "                sigma_fuse=args.sigma_fuse,\n",
        "                rank_align=args.rank_align,\n",
        "            )\n",
        "            raw_linear.to(\"cpu\")\n",
        "        setattr(info[\"father\"], info[\"name\"], svd_linear)\n",
        "        # print(f\"decompose {info['full_name']} with ratio {param_ratio}\")\n",
        "    ed = time.time()\n",
        "    print(f\"decompose time: {ed-st}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245,
          "referenced_widgets": [
            "9e64961d68c64a7ab088e6dc13e61246",
            "985f1feefb3a4b8baa7148e331b25b64",
            "607d7c0eaa05437b89df16ad8beeca5b",
            "eeb313c55c594acc82eb2cd4e3423a90",
            "f79324c4913e48dca8690f940f4becf0",
            "5decea07d8314fc381b7bf5333583e99",
            "6129671f93aa484f9e10e5c4de6a3816",
            "a2b013a32eeb47a6bbe2a4de59ce00d4",
            "160e5a6cf3584226832f9a681c20fbb9",
            "8daf79e4cbec42cbbba1cedd64903c12",
            "0eaa2520dfc147b69749eb481780153e"
          ]
        },
        "id": "tmtA4wu0FkMo",
        "outputId": "a713cf22-5131-4585-bbb1-4c202ae17f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== target: ppl=40, ratio_target=0.5 ===\n",
            "low=0 mid=218, high=437, ppl=96.69224548339844, param_ratio=0.863457330415755\n",
            "low=219 mid=328, high=437, ppl=48.57245635986328, param_ratio=0.9360254625024865\n",
            "low=329 mid=383, high=437, ppl=42.33979797363281, param_ratio=0.9661030435647502\n",
            "low=384 mid=410, high=437, ppl=41.162567138671875, param_ratio=0.9804257012134473\n",
            "low=411 mid=424, high=437, ppl=40.30240249633789, param_ratio=0.9923612492540281\n",
            "low=425 mid=431, high=437, ppl=40.2237663269043, param_ratio=0.996180624627014\n",
            "low=432 mid=434, high=437, ppl=39.94973373413086, param_ratio=0.9980903123135069\n",
            "low=432 mid=433, high=434, ppl=40.21885681152344, param_ratio=0.9976128903918837\n",
            "=== Searching done, decomposing layers... ===\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e64961d68c64a7ab088e6dc13e61246",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/73 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decompose time: 0.14230942726135254\n"
          ]
        }
      ],
      "source": [
        "binary_search_truncation_rank(model, sensitivity, calib_loader, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKYwWoDyHzpF"
      },
      "source": [
        "#5. Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "b9b0ab44d935482885620b0b80b8fd0a",
            "b68a1018dc6d4c739ea0043f01ac50fa",
            "8bbce546689b4e649b2951dcbf9ac382",
            "51c3be9ebe0b434291b987de64170d9d",
            "aba2c5ea96194b8480cb81293a9a7414",
            "1a3607ca692d4b5aaeb2cac635c16076",
            "eaeacf2d3bbd44feafd571afde1adca1",
            "b0a01d3feff14f2bbd007bd7acb4070d",
            "d9d400a6d0e04043b7e87cc0748b877b",
            "a7866566a4d0489cb0df80acd13637c5",
            "42bf0007fc824cd6a3366097463c14bf"
          ]
        },
        "id": "9X6VztRGL1Yd",
        "outputId": "ffa5b807-2312-4e95-faac-6e5fc17eaec3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9b0ab44d935482885620b0b80b8fd0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/140 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wikitext2 27.796283721923828\n",
            "{'wikitext2': 27.796283721923828}\n"
          ]
        }
      ],
      "source": [
        "result = evaluate_model(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        args.model_id,\n",
        "        eval_ppl=args.eval_ppl,\n",
        "        limit=-1,\n",
        "        use_bos=args.use_bos,\n",
        "    )\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFXWsdj-MXwE",
        "outputId": "cd991e0f-a451-4a43-9d2d-a2106e3d63ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
              "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): SVDLinear(\n",
              "              (ALinear): Linear(in_features=307, out_features=768, bias=True)\n",
              "              (BLinear): Linear(in_features=307, out_features=768, bias=False)\n",
              "            )\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): SVDLinear(\n",
              "              (ALinear): Linear(in_features=345, out_features=768, bias=True)\n",
              "              (BLinear): Linear(in_features=345, out_features=768, bias=False)\n",
              "            )\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): SVDLinear(\n",
              "              (ALinear): Linear(in_features=345, out_features=768, bias=True)\n",
              "              (BLinear): Linear(in_features=345, out_features=768, bias=False)\n",
              "            )\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4-8): 5 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): SVDLinear(\n",
              "              (ALinear): Linear(in_features=345, out_features=768, bias=True)\n",
              "              (BLinear): Linear(in_features=345, out_features=768, bias=False)\n",
              "            )\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10-11): 2 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dliIEyr6RkMh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0eaa2520dfc147b69749eb481780153e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "160e5a6cf3584226832f9a681c20fbb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a3607ca692d4b5aaeb2cac635c16076": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2464a177884f4565a00e2bd5e6a1b308": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24d4de4c7f384524a9bba3bd466b66c7",
            "placeholder": "​",
            "style": "IPY_MODEL_416cc21dbd814802a11150d75b7dfa8f",
            "value": "100%"
          }
        },
        "24d4de4c7f384524a9bba3bd466b66c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c0768d72c0a4bdc845a0d43dc9dbdb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "416cc21dbd814802a11150d75b7dfa8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42bf0007fc824cd6a3366097463c14bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51c3be9ebe0b434291b987de64170d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7866566a4d0489cb0df80acd13637c5",
            "placeholder": "​",
            "style": "IPY_MODEL_42bf0007fc824cd6a3366097463c14bf",
            "value": " 140/140 [00:05&lt;00:00, 24.72it/s]"
          }
        },
        "5decea07d8314fc381b7bf5333583e99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "607d7c0eaa05437b89df16ad8beeca5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2b013a32eeb47a6bbe2a4de59ce00d4",
            "max": 73,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_160e5a6cf3584226832f9a681c20fbb9",
            "value": 73
          }
        },
        "6129671f93aa484f9e10e5c4de6a3816": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "772ed6cb50ed4bd885e1cd43fdd5f312": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bbce546689b4e649b2951dcbf9ac382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0a01d3feff14f2bbd007bd7acb4070d",
            "max": 140,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9d400a6d0e04043b7e87cc0748b877b",
            "value": 140
          }
        },
        "8daf79e4cbec42cbbba1cedd64903c12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dc70d80618b4bc7a5ed241d6146d47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "918191b03b344e79b351cfc01bbe2804": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2464a177884f4565a00e2bd5e6a1b308",
              "IPY_MODEL_ac41383d3a5f46e3882417bbd8aca034",
              "IPY_MODEL_b8c7460d407948a2a04ab4373969482e"
            ],
            "layout": "IPY_MODEL_772ed6cb50ed4bd885e1cd43fdd5f312"
          }
        },
        "985f1feefb3a4b8baa7148e331b25b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5decea07d8314fc381b7bf5333583e99",
            "placeholder": "​",
            "style": "IPY_MODEL_6129671f93aa484f9e10e5c4de6a3816",
            "value": "100%"
          }
        },
        "9e64961d68c64a7ab088e6dc13e61246": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_985f1feefb3a4b8baa7148e331b25b64",
              "IPY_MODEL_607d7c0eaa05437b89df16ad8beeca5b",
              "IPY_MODEL_eeb313c55c594acc82eb2cd4e3423a90"
            ],
            "layout": "IPY_MODEL_f79324c4913e48dca8690f940f4becf0"
          }
        },
        "9f57359032d74a87b6a0103d2cd6e4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2b013a32eeb47a6bbe2a4de59ce00d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7866566a4d0489cb0df80acd13637c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aba2c5ea96194b8480cb81293a9a7414": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac41383d3a5f46e3882417bbd8aca034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec67c4a1f82a45458707e7562c67f576",
            "max": 140,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f57359032d74a87b6a0103d2cd6e4f1",
            "value": 140
          }
        },
        "b0a01d3feff14f2bbd007bd7acb4070d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68a1018dc6d4c739ea0043f01ac50fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a3607ca692d4b5aaeb2cac635c16076",
            "placeholder": "​",
            "style": "IPY_MODEL_eaeacf2d3bbd44feafd571afde1adca1",
            "value": "100%"
          }
        },
        "b8c7460d407948a2a04ab4373969482e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c0768d72c0a4bdc845a0d43dc9dbdb8",
            "placeholder": "​",
            "style": "IPY_MODEL_8dc70d80618b4bc7a5ed241d6146d47c",
            "value": " 140/140 [00:07&lt;00:00, 22.26it/s]"
          }
        },
        "b9b0ab44d935482885620b0b80b8fd0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b68a1018dc6d4c739ea0043f01ac50fa",
              "IPY_MODEL_8bbce546689b4e649b2951dcbf9ac382",
              "IPY_MODEL_51c3be9ebe0b434291b987de64170d9d"
            ],
            "layout": "IPY_MODEL_aba2c5ea96194b8480cb81293a9a7414"
          }
        },
        "d9d400a6d0e04043b7e87cc0748b877b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eaeacf2d3bbd44feafd571afde1adca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec67c4a1f82a45458707e7562c67f576": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb313c55c594acc82eb2cd4e3423a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8daf79e4cbec42cbbba1cedd64903c12",
            "placeholder": "​",
            "style": "IPY_MODEL_0eaa2520dfc147b69749eb481780153e",
            "value": " 73/73 [00:00&lt;00:00, 523.39it/s]"
          }
        },
        "f79324c4913e48dca8690f940f4becf0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
